{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgOhk8U4Rpl"
      },
      "source": [
        "# Quickstart: Querying PDF With Astra and LangChain\n",
        "\n",
        "### A question-answering demo using Astra DB and LangChain, powered by Vector Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfJKgRM4Rpo"
      },
      "source": [
        "#### Pre-requisites:\n",
        "\n",
        "You need a **_Serverless Cassandra with Vector Search_** database on [Astra DB](https://astra.datastax.com) to run this demo. As outlined in more detail [here](https://docs.datastax.com/en/astra-serverless/docs/vector-search/quickstart.html#_prepare_for_using_your_vector_database), you should get a DB Token with role _Database Administrator_ and copy your Database ID: these connection parameters are needed momentarily.\n",
        "\n",
        "You also need an [OpenAI API Key](https://cassio.org/start_here/#llm-access) for this demo to work.\n",
        "\n",
        "#### What you will do:\n",
        "\n",
        "- Setup: import dependencies, provide secrets, create the LangChain vector store;\n",
        "- Run a Question-Answering loop retrieving the relevant headlines and having an LLM construct the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_FeN-Ep4Rpp"
      },
      "source": [
        "Install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk0qUhJUQrkO"
      },
      "outputs": [],
      "source": [
        "%pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQN-L2J4Rpq"
      },
      "source": [
        "Import the packages you'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4qBIihE4Rpq"
      },
      "outputs": [],
      "source": [
        "# LangChain components to use\n",
        "from langchain_classic.vectorstores.cassandra import Cassandra\n",
        "from langchain_classic.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain_classic.llms import OpenAI\n",
        "from langchain_classic.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Support for dataset retrieval with Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# With Cassio, the engine powering the Astra DB integration in LangChain,\n",
        "# you will also initialize the DB connection:\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs76OPQ6JyD",
        "outputId": "2464981f-aea9-499d-ccb4-5f43ea76061d"
      },
      "outputs": [],
      "source": [
        "%pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1itBNL1v6N9-"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu2UauiC4Rpr"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1cmD5EF4Rpr"
      },
      "source": [
        "#### Provide your secrets:\n",
        "\n",
        "Replace the following with your Astra DB connection details and your OpenAI API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
        "ASTRA_DB_ID = os.getenv(\"ASTRA_DB_ID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waVKJW-n6jqJ"
      },
      "outputs": [],
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('Sanaz.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42BKuFRO6meP"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "vR41Iq-4ZHnG",
        "outputId": "861bc27a-fd4d-47f9-f722-8e365a6fd030"
      },
      "outputs": [],
      "source": [
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0GgIQs4Rps"
      },
      "source": [
        "Initialize the connection to your database:\n",
        "\n",
        "_(do not worry if you see a few warnings, it's just that the drivers are chatty about negotiating protocol versions with the DB.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFBR5HnZSPmK",
        "outputId": "5b4bb3ea-6be3-4d7a-c535-88715fa67c13"
      },
      "outputs": [],
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7NxZYb4Rps"
      },
      "source": [
        "Create the LangChain embedding and LLM objects for later usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TavS0AK2SLrL"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HMMx5Pm4Rpt"
      },
      "source": [
        "Create your LangChain vector store ... backed by Astra DB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg9VAk4USQvU"
      },
      "outputs": [],
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FMAhKr77AVO"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.text_splitter import CharacterTextSplitter\n",
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8BDHAyT7Gjr",
        "outputId": "7833f6ac-bd97-40d6-fcbe-94a81b4dd6ac"
      },
      "outputs": [],
      "source": [
        "texts[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1WK54-74Rpt"
      },
      "source": [
        "### Load the dataset into the vector store\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5BECsdSUUM",
        "outputId": "cdff3467-8af3-45cd-f750-f3174bc521fb"
      },
      "outputs": [],
      "source": [
        "# Take the top 50 texts\n",
        "astra_vector_store.add_texts(texts[:50])\n",
        "\n",
        "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJp8yPF4Rpt"
      },
      "source": [
        "### Run the QA cycle\n",
        "\n",
        "Simply run the cells and ask a question -- or `quit` to stop. (you can also stop execution with the \"â–ª\" button on the top toolbar)\n",
        "\n",
        "Here are some suggested questions:\n",
        "- _Who is Sanaz?_\n",
        "- _What is her degree about?_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJugrh7SX3C",
        "outputId": "10e8f954-a113-47a2-a84c-615a9f6e5dc6"
      },
      "outputs": [],
      "source": [
        "first_question = True\n",
        "while True:\n",
        "    if first_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    if query_text == \"\":\n",
        "        continue\n",
        "\n",
        "    first_question = False\n",
        "\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSaUPguw389l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
